{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Authors: Rahul Sawhney, Nikhil Kumar Pradhan, Amit Kumar\n",
    "Project Control-Flow: 1) class Dataset\n",
    "                                : __init__\n",
    "                                : __len__\n",
    "                                : __getitem__\n",
    "                                :@ normalize\n",
    "                                :@ get_data\n",
    "                      2) class DataAnalysis:\n",
    "                                :@ Images after applying 7-8 different convolutional kernels  \n",
    "                      3) class Preprocess\n",
    "                                :...\n",
    "                      4) class GPU_Acceleration\n",
    "                                :@ hardware_acceleration\n",
    "                                :@ set_GPU\n",
    "                                :@ load_GPU\n",
    "                                :@ is_working_GPU\n",
    "                                : __init__\n",
    "                                : __len__\n",
    "                                : __iter__\n",
    "                      5) class Hyperparams\n",
    "                                : >Layers \n",
    "                                : >Neurons Engineering\n",
    "                                : >Activation Functions \n",
    "                      6) class CNNModel\n",
    "                                :__init__\n",
    "                                :@ forward\n",
    "                      7) class TrainCNN\n",
    "                                : train_dataset --> split() --> 75% ad 25% as training and validate set \n",
    "                      8) class TestCNN\n",
    "                                : Model on New unseen Data\n",
    "                      9) class EvaluateCNN\n",
    "                                :@ plot_1\n",
    "                                :@ plot_2\n",
    "                                :@ plot_3\n",
    "                                :@ plot_4\n",
    "                                :@ plot_5\n",
    "                                :@ plot_6 \n",
    "                      10) class SaveCNN := BrainMRI_MODEL.pt\n",
    "                                :@ save_CNN\n",
    "                                :@ is_save_CNN\n",
    "                                :@ load_CNN\n",
    "                                :@ is_load_CNN\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri\n",
    "\n",
    "\n",
    "Project Abstract:\n",
    "A Brain tumor is considered as one of the aggressive diseases, among children and adults. Brain tumors account for 85 to 90 percent of all primary Central Nervous \n",
    "System(CNS) tumors. Every year, around 11,700 people are diagnosed with a brain tumor. The 5-year survival rate for people with a cancerous brain or CNS tumor is \n",
    "approximately 34 percent for men and36 percent for women. Brain Tumors are classified as: Benign Tumor, Malignant Tumor, Pituitary Tumor, etc. \n",
    "Proper treatment, planning, and accurate diagnostics should be implemented to improve the life expectancy of the patients. The best technique to \n",
    "detect brain tumors is Magnetic Resonance Imaging (MRI). A huge amount of image data is generated through the scans. These images are examined by the radiologist. \n",
    "A manual examination can be error-prone due to the level of complexities involved in brain tumors and their properties.\n",
    "Application of automated classification techniques using Machine Learning(ML) and Artificial Intelligence(AI)has consistently shown higher accuracy than manual \n",
    "classification. Hence, proposing a system performing detection and classification by using Deep Learning Algorithms using ConvolutionNeural Network (CNN), \n",
    "Artificial Neural Network (ANN), and TransferLearning (TL) would be helpful to doctors all around the world.\n",
    "\n",
    "\n",
    "Context:\n",
    "Brain Tumors are complex. There are a lot of abnormalities in the sizes and location of the brain tumor(s). This makes it really difficult for complete \n",
    "understanding of the nature of the tumor. Also, a professional Neurosurgeon is required for MRI analysis. Often times in developing countries the lack of\n",
    "skillful doctors and lack of knowledge about tumors makes it really challenging and time-consuming to generate reports from MRIâ€™. So an automated system on Cloud \n",
    "can solve this problem.\n",
    "                      \n",
    "\"\"\"\n",
    "\n",
    "# py imports\n",
    "from __future__ import annotations\n",
    "import typing\n",
    "typing.__name__ = \"__main_module__\"\n",
    "from typing import Any, NewType, Generator, Optional, Union\n",
    "import os, warnings\n",
    "warnings.filterwarnings(action= 'ignore')\n",
    "\n",
    "\n",
    "# Data Analysis Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Scripting ML \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# DL imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "# torch typing scripts\n",
    "_path =  NewType(\"_path\", Any)\n",
    "_transform = NewType(\"_transform\", Any)\n",
    "_img = NewType(\"_img\", Any)\n",
    "_criterion = NewType(\"_criterion\", Any)\n",
    "_optimizor = NewType(\"_optimizor\", Any)\n",
    "_loss = NewType(\"_loss\", Any)\n",
    "_layer = NewType(\"_layer\", Any)\n",
    "_activation = NewType(\"_activation\", Any)\n",
    "_text = NewType(\"_text\", Any)\n",
    "_plot = NewType(\"_plot\", Any)\n",
    "_loader = NewType(\"_loader\", Any)\n",
    "\n",
    "\n",
    "#...DIR ...\n",
    "# Testing:    \n",
    "#        glioma_tumor \n",
    "#        meningioma_tumor \n",
    "#        no_tumor\n",
    "#        pituitary_tumor\n",
    "#\n",
    "# Training: \n",
    "#        glioma_tumor \n",
    "#        meningioma_tumor \n",
    "#        no_tumor\n",
    "#        pituitary_tumor\n",
    "#...\n",
    "\n",
    "# Class batches DataLoader Iterator:\n",
    "#@ Unit test Pass  \n",
    "class BrainMRIDataset(Dataset):\n",
    "    '''\n",
    "        A Custom `DataLoader Iterator` Class to load the set of Images in a `batch` \n",
    "        and applies some kind of `transformation` to every set of image in a batch of `Iterators`.\n",
    "\n",
    "        INIT Args:\n",
    "            path: _path     = Path of the Image Dataset folder\n",
    "            sub_path: str   = str <Training | Testing | Validation>\n",
    "            \n",
    "        OPTIONAL Args:\n",
    "            batch_size: int = size of the batches in the Training and Testing Dataset\n",
    "            img_resolution: Optional[int]   = resolution of the image to load. :Options = [8, 16, 32, 64, 128] pixels\n",
    "            transform: Optional[_transform] = `Sequencial Container` of various feature engineering teachiques.\n",
    "        \n",
    "        EXAMPLES:\n",
    "            >>> df_train: pd.DataFrame = BRAINDataset(path= .../BRAIN_TUMOR_MRI/, \n",
    "                                                      sub_path= \"Training\",\n",
    "                                                      batch_size= 8,\n",
    "                                                      img_resolution= 32,\n",
    "                                                      transform= None)\n",
    "            \n",
    "            >>> df_test: pd.DataFrame = BRAINDataset(path= .../BRAIN_TUMOR_MRI/,\n",
    "                                                     sub_path= \"Testing\",\n",
    "                                                     batch_size= 1,\n",
    "                                                     img_resolution= 32,\n",
    "                                                     transform= None)\n",
    "\n",
    "\n",
    "        UNIT TEST: \n",
    "            >>> path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "            >>> sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "            >>> df_train = BrainMRIDataset(path= path, \n",
    "                                           sub_path= sub_path[1],\n",
    "                                           batch_size= 8)\n",
    "    \n",
    "            >>> df_test = BrainMRIDataset(path= path, \n",
    "                                          sub_path= sub_path[0],\n",
    "                                          batch_size= 1)\n",
    "    \n",
    "            >>> print(df_test)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, path: _path, sub_path: str, batch_size: Optional[int] = 4, img_resolution: Optional[int] = 64, transform: Optional[_transform] = None) -> None:\n",
    "        self.path = path\n",
    "        self.sub_path = sub_path\n",
    "        self.batch_size = batch_size\n",
    "        self.img_resolution = img_resolution\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        self.categories: list[str] = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
    "\n",
    "        if sub_path == 'Training':\n",
    "            self.dataset: pd.DataFrame = self.get_data(path, 'Training', self.categories)\n",
    "        if sub_path == 'Testing':\n",
    "            self.dataset: pd.DataFrame = self.get_data(path, 'Testing', self.categories)\n",
    "        \n",
    "        indexes: list[int] = [x for x in range(len(self.dataset))]\n",
    "        \n",
    "        self.index_batch: list[list[int]] = [\n",
    "            indexes[i : i + batch_size]\n",
    "            for i in range(0, len(indexes), batch_size)\n",
    "        ]\n",
    "        \n",
    "    \n",
    "\n",
    "    def __len__(self) -> tuple[int, ...]:\n",
    "        return len(self.index_batch)\n",
    "    \n",
    "\n",
    " \n",
    "    def __getitem__(self, index: int) -> dict[_img, str]:\n",
    "        batch: list[int] = self.index_batch[index]\n",
    "        size: tuple[int, ...] = (self.img_resolution, self.img_resolution)\n",
    "        images: list[int] = []\n",
    "        labels: list[str] = []\n",
    "        for i in batch:\n",
    "            img: _img = Image.open(self.dataset.iloc[i].path).convert('LA').resize(size)\n",
    "            img: np.ndarray = np.array(img)\n",
    "            lbl: list[str] = [self.dataset.iloc[i].label]\n",
    "            images.append(img)\n",
    "            labels.append(np.array(lbl))\n",
    "        \n",
    "        images: torch.Tensor = torch.tensor(images).type(torch.float32)\n",
    "        images: torch.Tensor = images.permute(0, 3, 1, 2)\n",
    "        labels: torch.Tensor = torch.tensor(labels).type(torch.float32)\n",
    "        images: torch.Tensor = self.normalize(images)\n",
    "        return images, labels\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x / 255\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_data(cls, path: _path, sub_path: str, categories: list[str]) -> pd.DataFrame:\n",
    "        glioma_tumor: _path     = path + sub_path + \"\\\\\" + categories[0] + \"\\\\\"\n",
    "        meningioma_tumor: _path = path + sub_path + \"\\\\\" + categories[1] + \"\\\\\"\n",
    "        no_tumor: _path         = path + sub_path + \"\\\\\" + categories[2] + \"\\\\\"\n",
    "        pituitary_tumor: _path  = path + sub_path + \"\\\\\" + categories[3] + \"\\\\\"\n",
    "\n",
    "        glioma_tumor_list: list[str]  = [\n",
    "            os.path.abspath(os.path.join(glioma_tumor, p))\n",
    "            for p in os.listdir(glioma_tumor)                            \n",
    "        ]\n",
    "        \n",
    "        meningioma_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(meningioma_tumor, p))\n",
    "            for p in os.listdir(meningioma_tumor)\n",
    "        ]\n",
    "\n",
    "        no_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(no_tumor, p))\n",
    "            for p in os.listdir(no_tumor)\n",
    "        ]  \n",
    "        \n",
    "\n",
    "        pituitary_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(pituitary_tumor, p))\n",
    "            for p in os.listdir(pituitary_tumor)\n",
    "        ]\n",
    "\n",
    "        glioma_tumor_labels: list[int]     = [0 for _ in range(len(glioma_tumor_list))]\n",
    "        meningioma_tumor_labels: list[int] = [1 for _ in range(len(meningioma_tumor_list))]\n",
    "        no_tumor_labels: list[int]         = [2 for _ in range(len(no_tumor_list))]\n",
    "        pituitary_tumor_labels: list[int]  = [3 for _ in range(len(pituitary_tumor_list))]\n",
    "\n",
    "        paths: _path      = glioma_tumor_list + meningioma_tumor_list + no_tumor_list + pituitary_tumor_list\n",
    "        labels: list[int] = glioma_tumor_labels + meningioma_tumor_labels + no_tumor_labels + pituitary_tumor_labels\n",
    "\n",
    "        dataframe: pd.DataFrame = pd.DataFrame.from_dict({'path': paths, 'label': labels})\n",
    "        dataframe: pd.DataFrame = dataframe.sample(frac= 1)\n",
    "        return dataframe \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@ nimkhil @Amitwa\n",
    "# Class Data Analysis\n",
    "class BRAINAnalysis:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot(cls) -> _img:\n",
    "        ...\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@ ... \n",
    "# Class Data Preprocess\n",
    "class BRAINPreprocess:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def something(cls) -> Any:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class GPU_acceleration\n",
    "class GPU_Acceleration:\n",
    "    def __init__(self, dataloader: _loader, device: Any) -> None:\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_default_device(cls) -> str:\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def to_device(cls, data: torch.Tensor, device: Any) -> Union[to_device, list]:\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return [to_device(x, device) for x in data]\n",
    "        return data.to(device, non_blocking= True)\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "\n",
    "\n",
    "    def __iter__(self) -> Generator[_loader, None, None]:\n",
    "        for data in self.dataloader:\n",
    "            yield self.to_device(data, self.device)\n",
    "      \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def is_working_GPU(cls) -> bool:\n",
    "        return True if cls.get_default_device() == 'cuda' else False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class Hyperparams\n",
    "#@ Unit TEST: Passed \n",
    "class CNNHyperParams(object):\n",
    "    '''\n",
    "        Params:\n",
    "            epochs: int           = No. of times the `Entire Dataset` is passed `Forward` abd `Backward` through the Neural Net\n",
    "            criterion: _criterion = Criterion which `Optimizes` a Multi-class hinde loss \n",
    "            optimizor: _optimizor = ...\n",
    "            learning_rate: float  = ...\n",
    "            weight_decay: float   = ...\n",
    "            momentum: float       = ...    \n",
    "\n",
    "        DL Layers:\n",
    "            convolution: _layer = Linear operation that involves the multiplication of a set of weights with the inpu\n",
    "            pooling: _layer     = ...\n",
    "            linear: _layer      = ...\n",
    "\n",
    "        Neurons Engineering:\n",
    "            flatten = ...\n",
    "            dropout = ...\n",
    "\n",
    "        Activation Functions:\n",
    "            relu: _activation    = ...\n",
    "            softmax: _activation = ...\n",
    "            tanh: _activation    = ...\n",
    "\n",
    "    '''\n",
    "    # HyperParams: @params\n",
    "class Hyperparams:\n",
    "    convolutional_1 = nn.Conv2d(in_channels= 2, out_channels= 32, kernel_size= (3, 3))\n",
    "    convolutional_2 = nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= (3, 3))\n",
    "    convolutional_3 = nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size= (3, 3))\n",
    "    convolutional_4 = nn.Conv2d(in_channels= 128, out_channels= 32, kernel_size= (3, 3))\n",
    "\n",
    "    pooling_1 = nn.MaxPool2d(kernel_size= (2, 2), stride= 2)\n",
    "    pooling_2 = nn.MaxPool2d(kernel_size= (2, 2), stride= 2)\n",
    "    pooling_3 = nn.MaxPool2d(kernel_size= (2, 2), stride= 2)\n",
    "    pooling_4 = nn.MaxPool2d(kernel_size= (2, 2), stride= 2)\n",
    "\n",
    "                            # 512\n",
    "    linear_1 = nn.Linear(in_features= 128, out_features= 64)\n",
    "    linear_2 = nn.Linear(in_features= 64, out_features= 32)\n",
    "    linear_3 = nn.Linear(in_features= 32, out_features= 16)\n",
    "    linear_4 = nn.Linear(in_features= 16, out_features= 8)\n",
    "    linear_5 = nn.Linear(in_features= 8, out_features= 4)\n",
    "\n",
    "    relu = nn.ReLU()\n",
    "    tanh = nn.Tanh()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    flatten = nn.Flatten()\n",
    "    dropout = nn.Dropout(p= 0.3, inplace= False)\n",
    "    \n",
    "\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            Hyperparams.convolutional_1,\n",
    "            Hyperparams.tanh,\n",
    "            Hyperparams.pooling_1,\n",
    "\n",
    "            Hyperparams.convolutional_2,\n",
    "            Hyperparams.tanh,\n",
    "            Hyperparams.pooling_2,\n",
    "\n",
    "            Hyperparams.convolutional_3,\n",
    "            Hyperparams.relu,\n",
    "            Hyperparams.pooling_3,\n",
    "\n",
    "            Hyperparams.convolutional_4,\n",
    "            Hyperparams.tanh,\n",
    "            Hyperparams.pooling_4\n",
    "            \n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            Hyperparams.dropout,\n",
    "            Hyperparams.linear_1,\n",
    "            Hyperparams.tanh,\n",
    "\n",
    "            Hyperparams.linear_2,\n",
    "            Hyperparams.relu,\n",
    "\n",
    "            Hyperparams.linear_3,\n",
    "            Hyperparams.relu,\n",
    "\n",
    "            Hyperparams.linear_4,\n",
    "            Hyperparams.tanh,\n",
    "\n",
    "            Hyperparams.linear_5,\n",
    "        \n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape)\n",
    "        x: torch.Tensor = self.convolution_layers(x)\n",
    "        #print(x.shape)            # 8, 2, 64, 64\n",
    "        #x: torch.Tensor = x.view(-1, 4 * 4 * 32)\n",
    "        #print(x.shape)\n",
    "        x = Hyperparams.flatten(x)\n",
    "        #print(x.shape)\n",
    "        #print(x.view(x.size(0), -1).shape)\n",
    "        #print(x.shape)\n",
    "        x: torch.Tensor = self.linear_layers(x)\n",
    "\n",
    "        #return Hyperparams.softmax(x)\n",
    "        return x\n",
    "\n",
    "# class train CNN\n",
    "class TrainCNN(object):\n",
    "    losses: list[float] = []\n",
    "\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def train(cls, model: _model, epochs: int,  x_train: torch.Tensor, \n",
    "                   y_train: torch.Tensor, optimizor: _optimizor, criterion: _criterion ) -> _text:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, model: _model, x_test: torch.Tensor, y_test: torch.Tensor, \n",
    "                  optimizor: _optimizor, criterion: _criterion) -> _text:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "    def get_losses(self) -> list[float]:\n",
    "        return self.losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CNN on test SET\n",
    "class TestCNN:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "    \n",
    "\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class Eval CNN\n",
    "class EvaluateCNN:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot_1(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot_2(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "   \n",
    "    @classmethod\n",
    "    def plot_3(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot_4(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot_5(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot_6(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "# class Save CNN\n",
    "# Trained Model Name := BrainMRI_MODEL.pt file\n",
    "class SaveCNN:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        info: list[str] = ['module', 'name', 'ObjectID']\n",
    "        return {item: value for item, value in zip(info, self.__repr__())} \n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, model: _model) -> None:\n",
    "        torch.save(model.state_dict(), 'BrainMRI_MODEL.pt')\n",
    "    \n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def is_save_model(cls, model: _model) -> bool:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, new_model: _model, old_model: str) -> _model:\n",
    "        new_model.load_state_dict(torch.load(old_model))\n",
    "        return new_model\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def is_load_model(cls, new_model: _model, old_model: _model) -> bool:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Driver code\n",
    "if __name__ == \"__main_module__\":\n",
    "    '''\n",
    "    path: _path = ...\n",
    "    sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "    \n",
    "\n",
    "    df_train: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[1],\n",
    "                                            batch_size= 6, \n",
    "                                            img_resolution= 64, \n",
    "                                            transforms= None)\n",
    "    \n",
    "    df_test: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[0],\n",
    "                                            batch_size= 6, \n",
    "                                            img_resolution= 64, \n",
    "                                            transforms= None)\n",
    "    \n",
    "\n",
    "    # train_loader: _loader = DataLoader(df_train, batch_size= 6, shuffle= True)\n",
    "    # test_loader: _loader = DataLoader(df_test, batch_size= 1, shuffle= False)\n",
    "    '''\n",
    "    #model: _model = CNNModel()\n",
    "    pass    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "df_train = BrainMRIDataset(path= path, \n",
    "                           sub_path= sub_path[1],\n",
    "                           batch_size= 8)\n",
    "\n",
    "df_test = BrainMRIDataset(path= path, \n",
    "                          sub_path= sub_path[0],\n",
    "                          batch_size= 1)\n",
    "\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNN_Model(\n  (convolution_layers): Sequential(\n    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): Tanh()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (4): Tanh()\n    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1))\n    (10): Tanh()\n    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (linear_layers): Sequential(\n    (0): Dropout(p=0.3, inplace=False)\n    (1): Linear(in_features=128, out_features=64, bias=True)\n    (2): Tanh()\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=32, out_features=16, bias=True)\n    (6): ReLU()\n    (7): Linear(in_features=16, out_features=8, bias=True)\n    (8): Tanh()\n    (9): Linear(in_features=8, out_features=4, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "model = CNN_Model(3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbls = df_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr= 0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "source": [
    "## Epochs and Final Trained Loss\n",
    "* Epochs : 20\n",
    "* Loss: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  1, Mean_Loss:0.0135504\n",
      "Epoch:  2, Mean_Loss:0.0135245\n",
      "Epoch:  3, Mean_Loss:0.0135219\n",
      "Epoch:  4, Mean_Loss:0.0135193\n",
      "Epoch:  5, Mean_Loss:0.0135147\n",
      "Epoch:  6, Mean_Loss:0.0134880\n",
      "Epoch:  7, Mean_Loss:0.0121094\n",
      "Epoch:  8, Mean_Loss:0.0094954\n",
      "Epoch:  9, Mean_Loss:0.0078197\n",
      "Epoch: 10, Mean_Loss:0.0070323\n",
      "Epoch: 11, Mean_Loss:0.0066319\n",
      "Epoch: 12, Mean_Loss:0.0060181\n",
      "Epoch: 13, Mean_Loss:0.0059906\n",
      "Epoch: 14, Mean_Loss:0.0059756\n",
      "Epoch: 15, Mean_Loss:0.0058669\n",
      "Epoch: 16, Mean_Loss:0.0056625\n",
      "Epoch: 17, Mean_Loss:0.0053652\n",
      "Epoch: 18, Mean_Loss:0.0048968\n",
      "Epoch: 19, Mean_Loss:0.0053689\n",
      "Epoch: 20, Mean_Loss:0.0052565\n"
     ]
    }
   ],
   "source": [
    "mean_loss = []\n",
    "for e in range(20):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    \n",
    "    for idx in range(len(df_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs, lbls = df_train[idx]\n",
    "        \n",
    "        out = model(imgs)\n",
    "        lbls = lbls.squeeze_()\n",
    "        lbls = lbls.to('cpu', dtype= torch.int64)\n",
    "        #out = out.float()\n",
    "        loss = criterion(out, lbls)\n",
    "    \n",
    "        epoch_loss.append(np.array(loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = np.array(epoch_loss)\n",
    "    mean_loss.append(np.mean(epoch_loss) / 100)\n",
    "    print('Epoch:{0:3d}, Mean_Loss:{1:1.7f}'.format(e+1, mean_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}