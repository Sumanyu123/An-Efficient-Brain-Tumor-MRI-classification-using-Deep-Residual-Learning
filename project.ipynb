{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu9-EZ-nJpwA"
      },
      "source": [
        "from __future__ import annotations\n",
        "__authors__: list[str] = ['Rahul_Sawhney', 'nikhil_kumar_pradhan']\n",
        "\n",
        "# Python Imports\n",
        "import typing\n",
        "from typing import Any, NewType, Generator, Optional, Union, ClassVar, Container\n",
        "import os, warnings, sys\n",
        "warnings.filterwarnings(action= 'ignore')\n",
        "\n",
        "\n",
        "# Data Analysis Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as tFF\n",
        "\n",
        "\n",
        "# DL Imports \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# torch typing scripts\n",
        "_path =  NewType('_path', Any)\n",
        "_transform = NewType('_transform', Any)\n",
        "_img_path = NewType('_img', Any)\n",
        "_criterion = NewType('_criterion', Any)\n",
        "_optimizer = NewType('_optimizer', Any)\n",
        "_loss = NewType('_loss', Any)\n",
        "_layer = NewType('_layer', Any)\n",
        "_activation = NewType('_activation', Any)\n",
        "_text = NewType('_text', Any)\n",
        "_plot = NewType('_plot', Any)\n",
        "_loader = NewType('_loader', Any)\n",
        "_recurse = NewType('_recurse', Any)\n",
        "\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGDB3MnTJ4z7"
      },
      "source": [
        "## _class DataSet_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJEdH-CoNA6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f36a12-63f2-4e7c-87ff-0062a04f09a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSQ2PhWBJzbo"
      },
      "source": [
        "#@: Class DataSet\n",
        "class BrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: _path, sub_path: str, \n",
        "                                    categories: list[str], \n",
        "                                    img_resolution: Optional[int] = 64,\n",
        "                                    transform: Optional[Container[Module[_transform]]] = None) -> None:\n",
        "        self.path = path\n",
        "        self.sub_path = sub_path\n",
        "        self.categories = categories\n",
        "        self.img_resolution = img_resolution\n",
        "        if transform:\n",
        "            self.transform = transform\n",
        "        self.dataset: pd.DataFrame[_img_path, int] = self.get_data(path, sub_path, self.categories)\n",
        "    \n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def get_data(cls, path: _path, sub_path: str, categories: list[str]) -> pd.DataFrame[_img_path, int]:\n",
        "        # #@: local path\n",
        "        # glioma_tumor: _path = path + sub_path + '\\\\' + categories[0] + '\\\\'\n",
        "        # meningioma_tumor: _path = path + sub_path + '\\\\' + categories[1] + '\\\\'\n",
        "        # no_tumor: _path = path + sub_path + '\\\\' + categories[2] + '\\\\'\n",
        "        # pituitary_tumor: _path = path + sub_path + '\\\\' + categories[3] + '\\\\'\n",
        "\n",
        "\n",
        "        ##@: colab path\n",
        "        glioma_tumor: _path =  '/content/drive/MyDrive/BRAIN_MRI/Testing/glioma_tumor'\n",
        "        meningioma_tumor: _path = '/content/drive/MyDrive/BRAIN_MRI/Testing/meningioma_tumor'\n",
        "        no_tumor: _path = '/content/drive/MyDrive/BRAIN_MRI/Testing/no_tumor'\n",
        "        pituitary_tumor: _path = '/content/drive/MyDrive/BRAIN_MRI/Testing/pituitary_tumor'\n",
        "    \n",
        "\n",
        "\n",
        "        glioma_tumor_img_path: list[_img_path] = [\n",
        "            os.path.abspath(os.path.join(glioma_tumor, p)) for p in os.listdir(glioma_tumor)\n",
        "        ] \n",
        "        meningioma_tumor_img_path: list[_img_path] = [\n",
        "            os.path.abspath(os.path.join(meningioma_tumor, p)) for p in os.listdir(meningioma_tumor)\n",
        "        ]\n",
        "        no_tumor_img_path: list[_img_path] = [\n",
        "            os.path.abspath(os.path.join(no_tumor, p)) for p in os.listdir(no_tumor)\n",
        "        ]\n",
        "        pituitary_tumor_img_path: list[_img_path] = [\n",
        "            os.path.abspath(os.path.join(pituitary_tumor, p)) for p in os.listdir(pituitary_tumor)\n",
        "        ]\n",
        "\n",
        "\n",
        "        glioma_tumor_label: list[int] = [0 for _ in range(len(glioma_tumor_img_path))]\n",
        "        meningioma_tumor_label: list[int] = [1 for _ in range(len(meningioma_tumor_img_path))]\n",
        "        no_tumor_label: list[int] = [2 for _ in range(len(no_tumor_img_path))]\n",
        "        pituitary_tumor_label: list[int] = [3 for _ in range(len(pituitary_tumor_img_path))]\n",
        "\n",
        "\n",
        "        # pd.Dataframe[_img_path, int]  \n",
        "        all_img_path: list[_img_path] = glioma_tumor_img_path + meningioma_tumor_img_path + no_tumor_img_path + pituitary_tumor_img_path\n",
        "        all_label: list[int] = glioma_tumor_label + meningioma_tumor_label + no_tumor_label + pituitary_tumor_label\n",
        "\n",
        "        dataframe: pd.DataFrame[_img_path, int] = pd.DataFrame.from_dict({'path' : all_img_path, 'label': all_label})\n",
        "        dataframe: pd.DataFrame[_img_path, int] = dataframe.sample(frac= 1)\n",
        "        return dataframe\n",
        "\n",
        "\n",
        "\n",
        "    def __repr__(self) -> str(dict[str, str]):\n",
        "        return str({\n",
        "            item: value for item, value in zip(['Module', 'Name', 'ObjectID'],\n",
        "                                               [self.__module__, type(self).__name__, hex(id(self))])\n",
        "        })\n",
        "\n",
        "    \n",
        "    __str__ = __repr__\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[_img, int]:\n",
        "        img_size: tuple[int, ...] = (self.img_resolution, self.img_resolution)\n",
        "        image: _img = Image.open(self.dataset.iloc[index].path).convert('LA').resize(img_size)\n",
        "        label: int = self.dataset.iloc[index].label \n",
        "        if self.transform:\n",
        "            image: torch.Tensor = self.transform(image)\n",
        "        \n",
        "        return image, label  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiGAN8QuKJM6"
      },
      "source": [
        "## _class DataAnalysis_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbO9mg0jKIVe"
      },
      "source": [
        "class BrainAnalysis:\n",
        "    labels_map: ClassVar[dict[int, str]] = {\n",
        "        0: 'Glioma Tumor',\n",
        "        1: 'Meningioma Tumor',\n",
        "        2: 'No Tumor',\n",
        "        3: 'Pituitary Tumor'\n",
        "    }\n",
        "\n",
        "    def __repr__(self) -> str(dict[str, str]):\n",
        "        return str({\n",
        "            item: value for item, value in zip(['Module', 'Name', 'ObjectID'],\n",
        "                                               [self.__module__, type(self).__name__, hex(id(self))])\n",
        "        })\n",
        "\n",
        "\n",
        "    __str__ = __repr__\n",
        "\n",
        "\n",
        "    \n",
        "    @classmethod\n",
        "    def batch_img_display(cls, training_data: object) -> _plot:\n",
        "        figure: plt.figure = plt.figure(figsize= (8, 8))\n",
        "        cols, rows = 3, 3\n",
        "        for i in range(1, cols * rows + 1):\n",
        "            sample_index: int = torch.randint(len(training_data), size= (1,)).item()\n",
        "            img, label = training_data[sample_index]\n",
        "            figure.add_subplot(rows, cols, i)\n",
        "            plt.title(cls.labels_map[label])\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.asarray(tFF.to_pil_image(img).convert('RGB')), cmap= 'gray')\n",
        "        plt.show() \n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def data_loader_img_display(cls, training_loader: _loader) -> _plot:\n",
        "        train_feature, train_label = iter(training_loader).__next__()\n",
        "        image: _img = np.asarray(tFF.to_pil_image(train_feature[0]).convert('RGB'))\n",
        "        label: str = train_label[0]\n",
        "        plt.title(cls.labels_map[int(label.numpy())])\n",
        "        plt.imshow(image, cmap= 'gray')\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz46e2xKKRA3"
      },
      "source": [
        "# _class Data Preprocess_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI-ZEw_6KPI3"
      },
      "source": [
        "class BrainPreprocess:\n",
        "    def __repr__(self) -> str(dict[str, str]):\n",
        "        return str({\n",
        "            item: value for item, value in zip(['Module', 'Name', 'ObjectID'],\n",
        "                                               [self.__module__, type(self).__name__, hex(id(self))])\n",
        "        })\n",
        "\n",
        "    __str__ = __repr__\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def train_transform_container(cls) -> Container[Module[_transform]]:\n",
        "        container: Container[Module[_transform]] = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop(256, padding= 4, padding_mode= 'reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        return container\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def test_transform_container(cls) -> Container[Module[_transform]]:\n",
        "        container: Container[Module[_transform]] = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        return container"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKg3UuvHKkCH"
      },
      "source": [
        "## _Class GPU Acceleration_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CwzeF7pKgm-"
      },
      "source": [
        "class GPU_Accerlaration:\n",
        "    def __init__(self, train_loader: _loader, device: str) -> None:\n",
        "        self.train_loader = train_loader\n",
        "        self.device = device\n",
        "\n",
        "    \n",
        "    def __repr__(self) -> str(dict[str, str]):\n",
        "        return str({\n",
        "            item: value for item, value in zip(['Module', 'Name', 'ObjectID'],\n",
        "                                               [self.__module__, type(self).__name__, hex(id(self))])\n",
        "        })\n",
        "\n",
        "    \n",
        "    __str__ = __repr__\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def to_device(cls, data: torch.Tensor, device: str) -> _recurse | [base, str]:\n",
        "        if isinstance(data, (list, tuple)):\n",
        "            return [cls.to_device(x, device) for x in data]\n",
        "        return data.to(device, non_blocking= True)\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.train_loader)\n",
        "\n",
        "\n",
        "    def __iter__(self) -> Generator[_loader, None, None]:\n",
        "        for batch in self.train_loader:\n",
        "            yield self.to_device(batch, self.device)\n",
        "    \n",
        "\n",
        "    @classmethod\n",
        "    def is_working_gpu(cls) -> bool:\n",
        "        return True if cls.get_default_device() == 'cuda' else False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799pB09eK_fT"
      },
      "source": [
        "## _class CNN Paramters_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-i73_CHKZPH"
      },
      "source": [
        "class CNNHyperParams:\n",
        "    #@: Hyper_params\n",
        "    epochs: int = 5\n",
        "    optimizer: _optimizor = torch.optim.Adam                \n",
        "    #learning_rate: float = 0.01\n",
        "    criterion: _criterion = nn.CrossEntropyLoss()\n",
        "    weight_decay: float = 0.01\n",
        "    momentum: float = 0.9\n",
        "\n",
        "\n",
        "    #@: Layers\n",
        "    convolutional_1: _layer = nn.Conv2d(in_channels= 2, out_channels= 32, \n",
        "                                        kernel_size= (3, 3), stride= 1, \n",
        "                                        padding= 0, dilation= 1, \n",
        "                                        groups= 1, bias= True, \n",
        "                                        padding_mode= 'zeros')\n",
        "    \n",
        "    convolutional_2: _layer = nn.Conv2d(in_channels= 32, out_channels= 64,\n",
        "                                        kernel_size= (3, 3), stride= 1, \n",
        "                                        padding= 0, dilation= 1, \n",
        "                                        groups= 1, bias= True, \n",
        "                                        padding_mode= 'zeros')                                          \n",
        "    \n",
        "\n",
        "    convolutional_3: _layer = nn.Conv2d(in_channels= 64, out_channels= 128,  \n",
        "                                        kernel_size= (3, 3), stride= 1, \n",
        "                                        padding= 0, dilation= 1, \n",
        "                                        groups= 1, bias= True, \n",
        "                                        padding_mode= 'zeros')\n",
        "    \n",
        "    convolutional_4: _layer = nn.Conv2d(in_channels= 128, out_channels= 256, \n",
        "                                        kernel_size= (3, 3), stride= 1, \n",
        "                                        padding= 0, dilation= 1, \n",
        "                                        groups= 1, bias= True, \n",
        "                                        padding_mode= 'zeros')\n",
        "\n",
        "    \n",
        "    pooling_1: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 1,        \n",
        "                                     padding= 0, dilation= 1,\n",
        "                                     return_indices= False)\n",
        "                                    \n",
        "    \n",
        "    pooling_2: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 1,\n",
        "                                     padding= 0, dilation= 1,\n",
        "                                     return_indices= False)\n",
        "                                     \n",
        "    \n",
        "\n",
        "    pooling_3: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 1,\n",
        "                                     padding= 0, dilation= 1,\n",
        "                                     return_indices= False)\n",
        "                                     \n",
        "\n",
        "    pooling_4: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 2, \n",
        "                                     padding= 0, dilation= 1,\n",
        "                                     return_indices= False)\n",
        "                                     \n",
        "\n",
        "                                            \n",
        "    linear_1: _layer = nn.Linear(in_features= 256 * 122 * 122, out_features= 64, bias= True)    \n",
        "    linear_2: _layer = nn.Linear(in_features= 64, out_features= 32, bias= True)                  \n",
        "    linear_3: _layer = nn.Linear(in_features= 32, out_features= 16, bias= True)                 \n",
        "    linear_4: _layer = nn.Linear(in_features= 16, out_features= 8, bias= True)                  \n",
        "    linear_5: _layer = nn.Linear(in_features= 8, out_features= 4, bias= True)                 \n",
        "\n",
        " \n",
        "    #@: Activation functions\n",
        "    relu: _activation = nn.ReLU()\n",
        "    softmax: _activation = nn.Softmax()\n",
        "    tanh: _activation = nn.Tanh()\n",
        "\n",
        "\n",
        "    #@: Neuron's Characteristics\n",
        "    flatten = nn.Flatten()                                                          \n",
        "    dropout = nn.Dropout(p= 0.3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBYqtxq8LQqW"
      },
      "source": [
        "## _Custom CNN Model_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PymRpwBlLUhg"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes: int) -> None:\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        self.convolution_layers = nn.Sequential(\n",
        "            CNNHyperParams.convolutional_1,\n",
        "            CNNHyperParams.relu,\n",
        "            CNNHyperParams.pooling_1,\n",
        "\n",
        "            CNNHyperParams.convolutional_2,\n",
        "            CNNHyperParams.relu,\n",
        "            CNNHyperParams.pooling_2,\n",
        "\n",
        "            CNNHyperParams.convolutional_3,\n",
        "            CNNHyperParams.relu,\n",
        "            CNNHyperParams.pooling_3,\n",
        "\n",
        "            CNNHyperParams.convolutional_4,\n",
        "            CNNHyperParams.relu,\n",
        "            CNNHyperParams.pooling_4\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            CNNHyperParams.linear_1,\n",
        "            CNNHyperParams.relu,\n",
        "\n",
        "            CNNHyperParams.linear_2,\n",
        "            CNNHyperParams.relu,\n",
        "\n",
        "            CNNHyperParams.linear_3,\n",
        "            CNNHyperParams.relu,\n",
        "\n",
        "            CNNHyperParams.linear_4,\n",
        "            CNNHyperParams.relu,\n",
        "\n",
        "            CNNHyperParams.linear_5\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        #print(x.shape)\n",
        "        x: torch.Tensor = self.convolution_layers(x)\n",
        "        #print(x.shape)\n",
        "        #x: torch.Tensor = CNNHyperParams.dropout(x)\n",
        "        #print(x.shape)\n",
        "        x: torch.Tensor = CNNHyperParams.flatten(x)\n",
        "        #print(x.shape)\n",
        "        x: torch.Tensor = self.linear_layers(x)\n",
        "        #print(x.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYuVsa0LfSr"
      },
      "source": [
        "## _Class Train_Validate_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jZf_aJ6LZrm"
      },
      "source": [
        "class Train_validate:\n",
        "    def __init__(self, model: _model, train_loader: _loader, \n",
        "                                      test_loader: _loader, \n",
        "                                      criterion: _criterion,\n",
        "                                      optimizer: _optimizor ) -> None:\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer \n",
        "    \n",
        "    \n",
        "\n",
        "    def __repr__(self) -> str(dict[str, str]):\n",
        "        return str({\n",
        "            item: value for item, value in zip(['Module', 'Name', 'ObjectID'], [self.__module__, type(self).__name__, hex(id(self))])\n",
        "        })\n",
        "\n",
        "\n",
        "    __str__ = __repr__\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def train_epoch(cls, model: _model, dataloader: _loader, \n",
        "                         optimizer: _optimizor, criterion: _criterion) -> tuple[float, float]:\n",
        "        model.train()\n",
        "        total_correct: int = 0\n",
        "        total_loss: float = 0.0\n",
        "        total_examples: int = 0\n",
        "        \n",
        "        for index, data in enumerate(dataloader):\n",
        "            X, y = data\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            y_hat = model(X.float())\n",
        "            loss = criterion(y_hat, y.long())\n",
        "            \n",
        "            #@: back propogation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #@: end back propogation\n",
        "\n",
        "            total_examples += y.size(0)\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (torch.argmax(y_hat, 1) == y).sum().item()\n",
        "\n",
        "        return total_loss / len(dataloader), total_correct / total_examples \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def test_epoch(cls, model: _model, dataloader: _loader, criterion: _criterion) -> tuple[float, float]:\n",
        "        model.eval()\n",
        "        total_correct: int = 0\n",
        "        total_examples: int = 0\n",
        "        total_loss: float = 0.0\n",
        "        \n",
        "        for index, data in enumerate(dataloader):\n",
        "            X, y = data\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            y_hat = model(X.float())\n",
        "            loss = criterion(y_hat, y.long())\n",
        "\n",
        "            total_examples += y.size(0)\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (torch.argmax(y_hat, 1) == y).sum().item()\n",
        "\n",
        "        return total_loss / len(dataloader), total_correct / total_examples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, epochs: int) -> _text:\n",
        "        total_train_accuracy: list[float] = []\n",
        "        total_test_accuracy: list[float] = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"------ Epoch {:02d} ------\".format(epoch))\n",
        "            \n",
        "            #@: training loss + accs\n",
        "            loss, train_acc = self.train_epoch(model= self.model, dataloader= tqdm(self.train_loader, desc= 'Training', unit= 'batch'), optimizer= self.optimizer, criterion= self.criterion)\n",
        "            total_train_accuracy.append(train_acc)\n",
        "            print(\"Train Loss: {:.04f}, Accuracy: {:.04f}\".format(loss, train_acc))\n",
        "            \n",
        "            #@: testing loss + accs\n",
        "            loss, test_acc = self.test_epoch(model= self.model, dataloader= self.test_loader, criterion= self.criterion)\n",
        "            total_test_accuracy.append(test_acc) \n",
        "            print(\"Test  Loss: {:.04f}, Accuracy: {:.04f}\".format(loss, test_acc))\n",
        "\n",
        "        #@: total accuracies:\n",
        "        print(f'Training Accuracy: {sum(total_train_accuracy) / len(total_train_accuracy)}')\n",
        "        print(f'Testing Accuracy: {sum(total_test_accuracy) / len(total_test_accuracy)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqeTyoBLpWy"
      },
      "source": [
        "# DRIVER CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tsEFE4vLrTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a572128-dba3-4dfe-a648-674cf4c87703"
      },
      "source": [
        "path: _path = '/content/drive/MyDrive/BRAIN_MRI/'\n",
        "sub_path: list[str] = ['Testing', 'Training']\n",
        "\n",
        "categories: list[str] = [\n",
        "    'glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'\n",
        "]\n",
        "\n",
        "\n",
        "training_data: object = BrainDataset(path= path,\n",
        "                                     sub_path= sub_path[1],\n",
        "                                     categories= categories,\n",
        "                                     img_resolution= 64,\n",
        "                                     transform= BrainPreprocess.train_transform_container())\n",
        "\n",
        "\n",
        "testing_data: object = BrainDataset(path= path,\n",
        "                                    sub_path= sub_path[0],\n",
        "                                    categories= categories, \n",
        "                                    img_resolution= 64,\n",
        "                                    transform= BrainPreprocess.test_transform_container())\n",
        "    \n",
        "    \n",
        "training_data_loader: _loader = DataLoader(dataset= training_data, \n",
        "                                           batch_size= 8, \n",
        "                                           shuffle= True, \n",
        "                                           #num_workers= 4,\n",
        "                                           pin_memory= True)\n",
        "\n",
        "\n",
        "testing_data_loader: _loader = DataLoader(dataset= testing_data,\n",
        "                                          batch_size= 8, \n",
        "                                          shuffle= False,\n",
        "                                          #num_workers= 2,\n",
        "                                          pin_memory= True)\n",
        "\n",
        "\n",
        "data_loader_dict: dict[str, object|_loader] = {\n",
        "    'training data'  : training_data,\n",
        "    'training loader': training_data_loader,\n",
        "    'testing data'   : testing_data,\n",
        "    'testing loader' : testing_data_loader\n",
        "}\n",
        "\n",
        "print(data_loader_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'training data': {'Module': '__main__', 'Name': 'BrainDataset', 'ObjectID': '0x7f2bcf3b14d0'}, 'training loader': <torch.utils.data.dataloader.DataLoader object at 0x7f2bcec121d0>, 'testing data': {'Module': '__main__', 'Name': 'BrainDataset', 'ObjectID': '0x7f2bcec12050'}, 'testing loader': <torch.utils.data.dataloader.DataLoader object at 0x7f2bcec12350>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZR-sbhDL3hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d577e18-92bc-4587-fa0e-fadc408a38d6"
      },
      "source": [
        "cnn_model = CNNModel(3)\n",
        "print(cnn_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNNModel(\n",
            "  (convolution_layers): Sequential(\n",
            "    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=3810304, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=8, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0bm86ioMITY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b479fd14-ccb0-4ae7-e37c-46937435faa5"
      },
      "source": [
        " def get_default_device() -> Any:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "DEVICE: Any = get_default_device()\n",
        "\n",
        "cnn_model.to(DEVICE)\n",
        "\n",
        "training_data_loader: _loader =  GPU_Accerlaration(training_data_loader, DEVICE)\n",
        "testing_data_loader: _loader = GPU_Accerlaration(testing_data_loader, DEVICE)\n",
        "\n",
        "gpu_loader_dict: dict[str, _loader] = {\n",
        "    'GPU Training DataLoader': training_data_loader, \n",
        "    'GPU Testing DataLoader': testing_data_loader,\n",
        "    'GPU': torch.cuda.get_device_name(0) \n",
        "}\n",
        "\n",
        "print(gpu_loader_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'GPU Training DataLoader': {'Module': '__main__', 'Name': 'GPU_Accerlaration', 'ObjectID': '0x7f2bcf3b1590'}, 'GPU Testing DataLoader': {'Module': '__main__', 'Name': 'GPU_Accerlaration', 'ObjectID': '0x7f2bcf3b1fd0'}, 'GPU': 'Tesla T4'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUbV1urhM2KL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c491ada-7985-461c-f22b-bc07cf400c7e"
      },
      "source": [
        "train_validate: object = Train_validate(model= cnn_model, \n",
        "                                        train_loader= training_data_loader, \n",
        "                                        test_loader= testing_data_loader, \n",
        "                                        criterion= CNNHyperParams.criterion, \n",
        "                                        optimizer= torch.optim.Adam(cnn_model.parameters(), lr= 0.1))\n",
        "\n",
        "train_validate.fit(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Epoch 00 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   2%|▏         | 1/50 [00:00<00:25,  1.89batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   4%|▍         | 2/50 [00:00<00:24,  1.99batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   6%|▌         | 3/50 [00:01<00:23,  2.03batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   8%|▊         | 4/50 [00:01<00:21,  2.09batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  10%|█         | 5/50 [00:02<00:21,  2.10batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  12%|█▏        | 6/50 [00:02<00:20,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  14%|█▍        | 7/50 [00:03<00:20,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  16%|█▌        | 8/50 [00:03<00:19,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  18%|█▊        | 9/50 [00:04<00:18,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  20%|██        | 10/50 [00:04<00:18,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  22%|██▏       | 11/50 [00:05<00:18,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  24%|██▍       | 12/50 [00:05<00:17,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  26%|██▌       | 13/50 [00:06<00:16,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  28%|██▊       | 14/50 [00:06<00:16,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  30%|███       | 15/50 [00:06<00:15,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  32%|███▏      | 16/50 [00:07<00:15,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  34%|███▍      | 17/50 [00:07<00:15,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  36%|███▌      | 18/50 [00:08<00:14,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  38%|███▊      | 19/50 [00:08<00:13,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  40%|████      | 20/50 [00:09<00:13,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  42%|████▏     | 21/50 [00:09<00:13,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  44%|████▍     | 22/50 [00:10<00:12,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  46%|████▌     | 23/50 [00:10<00:12,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  48%|████▊     | 24/50 [00:11<00:11,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  50%|█████     | 25/50 [00:11<00:11,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  52%|█████▏    | 26/50 [00:11<00:10,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  54%|█████▍    | 27/50 [00:12<00:10,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  56%|█████▌    | 28/50 [00:12<00:09,  2.25batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  58%|█████▊    | 29/50 [00:13<00:09,  2.27batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  60%|██████    | 30/50 [00:13<00:08,  2.26batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  62%|██████▏   | 31/50 [00:14<00:08,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  64%|██████▍   | 32/50 [00:14<00:08,  2.24batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  66%|██████▌   | 33/50 [00:15<00:07,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  68%|██████▊   | 34/50 [00:15<00:07,  2.25batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  70%|███████   | 35/50 [00:15<00:06,  2.25batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  72%|███████▏  | 36/50 [00:16<00:06,  2.25batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  74%|███████▍  | 37/50 [00:16<00:05,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  76%|███████▌  | 38/50 [00:17<00:05,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  78%|███████▊  | 39/50 [00:17<00:05,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  80%|████████  | 40/50 [00:18<00:04,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  82%|████████▏ | 41/50 [00:18<00:04,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  84%|████████▍ | 42/50 [00:19<00:03,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  86%|████████▌ | 43/50 [00:19<00:03,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  88%|████████▊ | 44/50 [00:20<00:02,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  90%|█████████ | 45/50 [00:20<00:02,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  92%|█████████▏| 46/50 [00:20<00:01,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  94%|█████████▍| 47/50 [00:21<00:01,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  96%|█████████▌| 48/50 [00:21<00:00,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  98%|█████████▊| 49/50 [00:22<00:00,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training: 100%|██████████| 50/50 [00:22<00:00,  2.23batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: nan, Accuracy: 0.2538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test  Loss: nan, Accuracy: 0.2538\n",
            "------ Epoch 01 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   2%|▏         | 1/50 [00:00<00:23,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   4%|▍         | 2/50 [00:00<00:22,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   6%|▌         | 3/50 [00:01<00:21,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   8%|▊         | 4/50 [00:01<00:21,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  10%|█         | 5/50 [00:02<00:20,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  12%|█▏        | 6/50 [00:02<00:20,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  14%|█▍        | 7/50 [00:03<00:19,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  16%|█▌        | 8/50 [00:03<00:19,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  18%|█▊        | 9/50 [00:04<00:19,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  20%|██        | 10/50 [00:04<00:18,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  22%|██▏       | 11/50 [00:05<00:18,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  24%|██▍       | 12/50 [00:05<00:17,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  26%|██▌       | 13/50 [00:05<00:17,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  28%|██▊       | 14/50 [00:06<00:16,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  30%|███       | 15/50 [00:06<00:16,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  32%|███▏      | 16/50 [00:07<00:15,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  34%|███▍      | 17/50 [00:07<00:15,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  36%|███▌      | 18/50 [00:08<00:14,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  38%|███▊      | 19/50 [00:08<00:14,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  40%|████      | 20/50 [00:09<00:14,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  42%|████▏     | 21/50 [00:09<00:13,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  44%|████▍     | 22/50 [00:10<00:13,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  46%|████▌     | 23/50 [00:10<00:12,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  48%|████▊     | 24/50 [00:11<00:11,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  50%|█████     | 25/50 [00:11<00:11,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  52%|█████▏    | 26/50 [00:11<00:10,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  54%|█████▍    | 27/50 [00:12<00:10,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  56%|█████▌    | 28/50 [00:12<00:10,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  58%|█████▊    | 29/50 [00:13<00:09,  2.10batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  60%|██████    | 30/50 [00:13<00:09,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  62%|██████▏   | 31/50 [00:14<00:08,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  64%|██████▍   | 32/50 [00:14<00:08,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  66%|██████▌   | 33/50 [00:15<00:07,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  68%|██████▊   | 34/50 [00:15<00:07,  2.24batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  70%|███████   | 35/50 [00:16<00:06,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  72%|███████▏  | 36/50 [00:16<00:06,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  74%|███████▍  | 37/50 [00:16<00:05,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  76%|███████▌  | 38/50 [00:17<00:05,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  78%|███████▊  | 39/50 [00:17<00:04,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  80%|████████  | 40/50 [00:18<00:04,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  82%|████████▏ | 41/50 [00:18<00:04,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  84%|████████▍ | 42/50 [00:19<00:03,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  86%|████████▌ | 43/50 [00:19<00:03,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  88%|████████▊ | 44/50 [00:20<00:02,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  90%|█████████ | 45/50 [00:20<00:02,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  92%|█████████▏| 46/50 [00:21<00:01,  2.11batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  94%|█████████▍| 47/50 [00:21<00:01,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  96%|█████████▌| 48/50 [00:22<00:00,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  98%|█████████▊| 49/50 [00:22<00:00,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training: 100%|██████████| 50/50 [00:22<00:00,  2.20batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: nan, Accuracy: 0.2538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test  Loss: nan, Accuracy: 0.2538\n",
            "------ Epoch 02 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   2%|▏         | 1/50 [00:00<00:23,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   4%|▍         | 2/50 [00:00<00:22,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   6%|▌         | 3/50 [00:01<00:21,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   8%|▊         | 4/50 [00:01<00:21,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  10%|█         | 5/50 [00:02<00:20,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  12%|█▏        | 6/50 [00:02<00:20,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  14%|█▍        | 7/50 [00:03<00:19,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  16%|█▌        | 8/50 [00:03<00:19,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  18%|█▊        | 9/50 [00:04<00:18,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  20%|██        | 10/50 [00:04<00:18,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  22%|██▏       | 11/50 [00:05<00:17,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  24%|██▍       | 12/50 [00:05<00:17,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  26%|██▌       | 13/50 [00:06<00:17,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  28%|██▊       | 14/50 [00:06<00:16,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  30%|███       | 15/50 [00:06<00:16,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  32%|███▏      | 16/50 [00:07<00:15,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  34%|███▍      | 17/50 [00:07<00:15,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  36%|███▌      | 18/50 [00:08<00:14,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  38%|███▊      | 19/50 [00:08<00:14,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  40%|████      | 20/50 [00:09<00:13,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  42%|████▏     | 21/50 [00:09<00:13,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  44%|████▍     | 22/50 [00:10<00:12,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  46%|████▌     | 23/50 [00:10<00:12,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  48%|████▊     | 24/50 [00:11<00:12,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  50%|█████     | 25/50 [00:11<00:11,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  52%|█████▏    | 26/50 [00:11<00:11,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  54%|█████▍    | 27/50 [00:12<00:10,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  56%|█████▌    | 28/50 [00:12<00:10,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  58%|█████▊    | 29/50 [00:13<00:09,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  60%|██████    | 30/50 [00:13<00:09,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  62%|██████▏   | 31/50 [00:14<00:08,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  64%|██████▍   | 32/50 [00:14<00:08,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  66%|██████▌   | 33/50 [00:15<00:07,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  68%|██████▊   | 34/50 [00:15<00:07,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  70%|███████   | 35/50 [00:16<00:07,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  72%|███████▏  | 36/50 [00:16<00:06,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  74%|███████▍  | 37/50 [00:17<00:05,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  76%|███████▌  | 38/50 [00:17<00:05,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  78%|███████▊  | 39/50 [00:17<00:05,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  80%|████████  | 40/50 [00:18<00:04,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  82%|████████▏ | 41/50 [00:18<00:04,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  84%|████████▍ | 42/50 [00:19<00:03,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  86%|████████▌ | 43/50 [00:19<00:03,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  88%|████████▊ | 44/50 [00:20<00:02,  2.23batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  90%|█████████ | 45/50 [00:20<00:02,  2.10batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  92%|█████████▏| 46/50 [00:21<00:01,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  94%|█████████▍| 47/50 [00:21<00:01,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  96%|█████████▌| 48/50 [00:22<00:00,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  98%|█████████▊| 49/50 [00:22<00:00,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training: 100%|██████████| 50/50 [00:22<00:00,  2.19batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: nan, Accuracy: 0.2538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test  Loss: nan, Accuracy: 0.2538\n",
            "------ Epoch 03 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   2%|▏         | 1/50 [00:00<00:21,  2.30batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   4%|▍         | 2/50 [00:00<00:21,  2.26batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   6%|▌         | 3/50 [00:01<00:21,  2.21batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:   8%|▊         | 4/50 [00:01<00:20,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  10%|█         | 5/50 [00:02<00:20,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  12%|█▏        | 6/50 [00:02<00:19,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  14%|█▍        | 7/50 [00:03<00:19,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  16%|█▌        | 8/50 [00:03<00:18,  2.22batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  18%|█▊        | 9/50 [00:04<00:18,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  20%|██        | 10/50 [00:04<00:18,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  22%|██▏       | 11/50 [00:05<00:17,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  24%|██▍       | 12/50 [00:05<00:18,  2.11batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  26%|██▌       | 13/50 [00:05<00:17,  2.11batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  28%|██▊       | 14/50 [00:06<00:16,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  30%|███       | 15/50 [00:06<00:16,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  32%|███▏      | 16/50 [00:07<00:16,  2.09batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  34%|███▍      | 17/50 [00:07<00:15,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  36%|███▌      | 18/50 [00:08<00:14,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  38%|███▊      | 19/50 [00:08<00:14,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  40%|████      | 20/50 [00:09<00:14,  2.11batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  42%|████▏     | 21/50 [00:09<00:13,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  44%|████▍     | 22/50 [00:10<00:12,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  46%|████▌     | 23/50 [00:10<00:12,  2.14batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  48%|████▊     | 24/50 [00:11<00:12,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  50%|█████     | 25/50 [00:11<00:11,  2.13batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  52%|█████▏    | 26/50 [00:12<00:11,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  54%|█████▍    | 27/50 [00:12<00:10,  2.16batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  56%|█████▌    | 28/50 [00:12<00:10,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  58%|█████▊    | 29/50 [00:13<00:09,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  60%|██████    | 30/50 [00:13<00:09,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  62%|██████▏   | 31/50 [00:14<00:08,  2.18batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  64%|██████▍   | 32/50 [00:14<00:08,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  66%|██████▌   | 33/50 [00:15<00:07,  2.19batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  68%|██████▊   | 34/50 [00:15<00:07,  2.20batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  70%|███████   | 35/50 [00:16<00:06,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  72%|███████▏  | 36/50 [00:16<00:06,  2.15batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  74%|███████▍  | 37/50 [00:17<00:06,  2.17batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  76%|███████▌  | 38/50 [00:17<00:05,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  78%|███████▊  | 39/50 [00:18<00:05,  2.12batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training:  80%|████████  | 40/50 [00:18<00:04,  2.11batch/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c05fd7b1cffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         optimizer= torch.optim.Adam(cnn_model.parameters(), lr= 0.1))\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_validate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-b689a4fd935f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#@: training loss + accs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mtotal_train_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Loss: {:.04f}, Accuracy: {:.04f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b689a4fd935f>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(cls, model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtotal_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX-ggq0PNZam"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}