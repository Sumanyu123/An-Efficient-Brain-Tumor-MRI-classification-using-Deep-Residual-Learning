{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Imports \n",
    "from __future__ import annotations\n",
    "import typing\n",
    "__name__ = typing.__name__ = \"__main_module__\", \"__main__\"\n",
    "from typing import Any, NewType, Generator, Optional, Union\n",
    "import os, warnings\n",
    "warnings.filterwarnings(action= 'ignore')\n",
    "\n",
    "\n",
    "# Data Analysis Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Scripting ML \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# DL imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# torch typing scripts\n",
    "_path =  NewType(\"_path\", Any)\n",
    "_transform = NewType(\"_transform\", Any)\n",
    "_img = NewType(\"_img\", Any)\n",
    "_criterion = NewType(\"_criterion\", Any)\n",
    "_optimizer = NewType(\"_optimizer\", Any)\n",
    "_loss = NewType(\"_loss\", Any)\n",
    "_layer = NewType(\"_layer\", Any)\n",
    "_activation = NewType(\"_activation\", Any)\n",
    "_text = NewType(\"_text\", Any)\n",
    "_plot = NewType(\"_plot\", Any)\n",
    "_loader = NewType(\"_loader\", Any)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## class DataSet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BrainMRIDataset object at 0x0000019B8E87ECD0> <BrainMRIDataset object at 0x0000019B8D7671F0>\n<torch.utils.data.dataloader.DataLoader object at 0x0000019B8D767070>\n<torch.utils.data.dataloader.DataLoader object at 0x0000019B8E87E0A0>\n"
     ]
    }
   ],
   "source": [
    "class BrainMRIDataset(Dataset):\n",
    "    def __init__(self, path: _path, \n",
    "                       sub_path: str, \n",
    "                       batch_size: Optional[int] = 4, \n",
    "                       img_resolution: Optional[int] = 64, \n",
    "                       transform: Optional[_transform] = None) -> None:\n",
    "        self.path = path\n",
    "        self.sub_path = sub_path\n",
    "        self.batch_size = batch_size\n",
    "        self.img_resolution = img_resolution\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        self.categories: list[str] = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
    "        if sub_path == 'Training':\n",
    "            self.dataset: pd.DataFrame = self.get_data(path, 'Training', self.categories)\n",
    "        if sub_path == 'Testing':\n",
    "            self.dataset: pd.DataFrame = self.get_data(path, 'Testing', self.categories)\n",
    "        indexes: list[int] = [x for x in range(len(self.dataset))]\n",
    "        self.index_batch: list[list[int]] = [\n",
    "            indexes[i : i + batch_size]\n",
    "            for i in range(0, len(indexes), batch_size)\n",
    "        ]\n",
    "        \n",
    "    \n",
    "    def __len__(self) -> tuple[int, ...]:\n",
    "        return len(self.index_batch)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict[_img, str]:\n",
    "        batch: list[int] = self.index_batch[index]\n",
    "        size: tuple[int, ...] = (self.img_resolution, self.img_resolution)\n",
    "        images: list[int] = []\n",
    "        labels: list[str] = []\n",
    "        for i in batch:\n",
    "            img: _img = Image.open(self.dataset.iloc[i].path).convert('LA').resize(size)\n",
    "            img: np.ndarray = np.array(img)\n",
    "            lbl: list[str] = [self.dataset.iloc[i].label]\n",
    "            images.append(img)\n",
    "            labels.append(np.array(lbl))\n",
    "        images: torch.Tensor = torch.tensor(images).type(torch.float32)\n",
    "        images: torch.Tensor = images.permute(0, 3, 1, 2)\n",
    "        labels: torch.Tensor = torch.tensor(labels).type(torch.float32)\n",
    "        images: torch.Tensor = self.normalize(images)\n",
    "        return images, labels\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x / 255\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_data(cls, path: _path, sub_path: str, categories: list[str]) -> pd.DataFrame:\n",
    "        glioma_tumor: _path     = path + sub_path + \"\\\\\" + categories[0] + \"\\\\\"\n",
    "        meningioma_tumor: _path = path + sub_path + \"\\\\\" + categories[1] + \"\\\\\"\n",
    "        no_tumor: _path         = path + sub_path + \"\\\\\" + categories[2] + \"\\\\\"\n",
    "        pituitary_tumor: _path  = path + sub_path + \"\\\\\" + categories[3] + \"\\\\\"\n",
    "        glioma_tumor_list: list[str]  = [\n",
    "            os.path.abspath(os.path.join(glioma_tumor, p))\n",
    "            for p in os.listdir(glioma_tumor)                            \n",
    "        ]\n",
    "        meningioma_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(meningioma_tumor, p))\n",
    "            for p in os.listdir(meningioma_tumor)\n",
    "        ]\n",
    "        no_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(no_tumor, p))\n",
    "            for p in os.listdir(no_tumor)\n",
    "        ]  \n",
    "        pituitary_tumor_list: list[str] = [\n",
    "            os.path.abspath(os.path.join(pituitary_tumor, p))\n",
    "            for p in os.listdir(pituitary_tumor)\n",
    "        ]\n",
    "        glioma_tumor_labels: list[int]     = [0 for _ in range(len(glioma_tumor_list))]\n",
    "        meningioma_tumor_labels: list[int] = [1 for _ in range(len(meningioma_tumor_list))]\n",
    "        no_tumor_labels: list[int]         = [2 for _ in range(len(no_tumor_list))]\n",
    "        pituitary_tumor_labels: list[int]  = [3 for _ in range(len(pituitary_tumor_list))]\n",
    "        paths: _path      = glioma_tumor_list + meningioma_tumor_list + no_tumor_list + pituitary_tumor_list\n",
    "        labels: list[int] = glioma_tumor_labels + meningioma_tumor_labels + no_tumor_labels + pituitary_tumor_labels\n",
    "        dataframe: pd.DataFrame = pd.DataFrame.from_dict({'path': paths, 'label': labels})\n",
    "        dataframe: pd.DataFrame = dataframe.sample(frac= 1)\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "\n",
    "# Driver code\n",
    "if __name__.__contains__('__main__'):\n",
    "    path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "    sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "    training_data: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[1],\n",
    "                                            batch_size= 8)\n",
    "\n",
    "    testing_data: object  = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[0],\n",
    "                                            batch_size= 1)\n",
    "\n",
    "    training_data_loader: _loader = DataLoader(training_data, \n",
    "                                               batch_size= 8,\n",
    "                                               shuffle= True)\n",
    "\n",
    "    testing_data_loader: _loader = DataLoader(testing_data, \n",
    "                                              batch_size= 1,\n",
    "                                              shuffle= False)\n",
    "    \n",
    "    print(training_data, testing_data)\n",
    "    print(training_data_loader)\n",
    "    print(testing_data_loader)\n"
   ]
  },
  {
   "source": [
    "## Class DataAnalysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Data Analysis \n",
    "class BRAINAnalysis:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        return {\n",
    "            item: value for item, value in zip(['module', 'name', 'ObjectID'], self.__repr__())\n",
    "        }\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def plot(cls) -> _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "# Driver code\n",
    "if __name__.__contains__('__main__'):\n",
    "    ..."
   ]
  },
  {
   "source": [
    "## Class DataPreprocess"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Data Preprocess\n",
    "class BRAINPreprocess:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "\n",
    "    \n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        return {\n",
    "            item: value for item, value in zip(['module', 'name', 'ObjectID'], self.__repr__())\n",
    "        }\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def something(cls) -> None: ...\n",
    "    \n",
    "    \n",
    "\n",
    "# Driver code\n",
    "if __name__.__contains__('__main__'):\n",
    "    ..."
   ]
  },
  {
   "source": [
    "## Class GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPU_acceleration\n",
    "class GPU_Acceleration:\n",
    "    def __init__(self, dataloader: _loader, device: Any) -> None:\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_default_device(cls) -> str:\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def to_device(cls, data: torch.Tensor, device: Any) -> Union[to_device, list]:\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return [to_device(x, device) for x in data]\n",
    "        return data.to(device, non_blocking= True)\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "\n",
    "    def __iter__(self) -> Generator[_loader, None, None]:\n",
    "        for data in self.dataloader:\n",
    "            yield self.to_device(data, self.device)\n",
    "      \n",
    "      \n",
    "    @classmethod\n",
    "    def is_working_GPU(cls) -> bool:\n",
    "        return True if cls.get_default_device() == 'cuda' else False\n",
    "\n",
    "\n",
    "# Driver code\n",
    "if __name__.__contains__('__main__'):\n",
    "    path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "    sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "    training_data: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[1],\n",
    "                                            batch_size= 8)\n",
    "\n",
    "    testing_data: object  = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[0],\n",
    "                                            batch_size= 1)\n",
    "\n",
    "    training_data_loader: _loader = DataLoader(training_data, \n",
    "                                               batch_size= 8,\n",
    "                                               shuffle= True)\n",
    "\n",
    "    testing_data_loader: _loader = DataLoader(testing_data, \n",
    "                                              batch_size= 1,\n",
    "                                              shuffle= False)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "## Image Classifier Base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classifier Base : -> Inherited : CNN_model\n",
    "class Image_Classifier_Base(nn.Module):\n",
    "    def accuracy(self, outputs: Any, labels: str) -> float:\n",
    "        _, preds = torch.max(outputs, dim= 1)\n",
    "        return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    \n",
    "\n",
    "    def training_step(self, batch: int) -> float:\n",
    "        images, labels = batch\n",
    "        #print(images.shape)\n",
    "        images = images.squeeze_()\n",
    "        \n",
    "        images = images.unsqueeze(dim= 1)\n",
    "        #images = images.squeeze(dim= 1)\n",
    "        #print(images.shape)\n",
    "\n",
    "        out: Any = self(images)\n",
    "        labels = torch.argmax(labels, dim= 2)\n",
    "        loss: _loss = F.cross_entropy(out, torch.max(labels, 1)[1])\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch: int) -> dict[str, float]:\n",
    "        images, labels = batch\n",
    "        #print(images.shape)\n",
    "        images = images.squeeze_()\n",
    "        #print(images.shape)\n",
    "        images = images.unsqueeze(dim = 0)\n",
    "        #print(images.shape)\n",
    "        out: Any = self(images)\n",
    "        #print(labels.shape)\n",
    "        labels = torch.argmax(labels, dim= 2)\n",
    "        #print(labels.shape)\n",
    "        #loss: _loss = F.cross_entropy( torch.max(out, 0)[1] , torch.max(labels, 0)[1] )\n",
    "        loss: _loss = F.cross_entropy(out, torch.max(labels, 1)[1])\n",
    "        acc: float = self.accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "\n",
    "    def validation_epoch_end(self, outputs: Any) -> dict[str, tuple[int|float, ...]]:\n",
    "        batch_losses: list = [\n",
    "            x['val_loss'] for x in outputs\n",
    "        ]\n",
    "        epoch_loss: Container = torch.stack(batch_losses).mean()\n",
    "        batch_accs: list = [\n",
    "            x['val_acc'] for x in outputs\n",
    "        ]\n",
    "        epoch_acc: float = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    \n",
    "    def epoch_end(self, epoch: int, result: float) -> _text:\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "                epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "source": [
    "## Class CNNHyperParams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHyperParams:\n",
    "    #@ params\n",
    "    epochs: int = 5\n",
    "    optimizer: _optimizor = torch.optim.Adam\n",
    "    learning_rate: float = 0.01\n",
    "    criterion: _criterion = nn.CrossEntropyLoss()\n",
    "    weight_decay: float = 0.01\n",
    "    momentum: float = 0.9\n",
    "\n",
    "    #@ Layers\n",
    "    convolutional_1: _layer = nn.Conv2d(in_channels= 2, out_channels= 32, \n",
    "                                        kernel_size= (3, 3), stride= 1, \n",
    "                                        padding= 0, dilation= 1, \n",
    "                                        groups= 1, bias= True, \n",
    "                                        padding_mode= 'zeros')\n",
    "    \n",
    "    convolutional_2: _layer = nn.Conv2d(in_channels= 32, out_channels= 64, \n",
    "                                        kernel_size= (3, 3), stride= 1, \n",
    "                                        padding= 0, dilation= 1, \n",
    "                                        groups= 1, bias= True, \n",
    "                                        padding_mode= 'zeros')\n",
    "    \n",
    "    convolutional_3: _layer = nn.Conv2d(in_channels= 64, out_channels= 128, \n",
    "                                        kernel_size= (3, 3), stride= 1, \n",
    "                                        padding= 0, dilation= 1, \n",
    "                                        groups= 1, bias= True, \n",
    "                                        padding_mode= 'zeros')\n",
    "    \n",
    "    convolutional_4: _layer = nn.Conv2d(in_channels= 128, out_channels= 32, \n",
    "                                        kernel_size= (3, 3), stride= 1, \n",
    "                                        padding= 0, dilation= 1, \n",
    "                                        groups= 1, bias= True, \n",
    "                                        padding_mode= 'zeros')\n",
    "\n",
    "    \n",
    "    pooling_1: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 2, \n",
    "                                     padding= 0, dilation= 1,\n",
    "                                     return_indices= False,\n",
    "                                     ceil_mode= False)\n",
    "    \n",
    "    pooling_2: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 2,\n",
    "                                     padding= 0, dilation= 1,\n",
    "                                     return_indices= False,\n",
    "                                     ceil_mode= False)\n",
    "    \n",
    "    pooling_3: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 2,\n",
    "                                     padding= 0, dilation= 1,\n",
    "                                     return_indices= False,\n",
    "                                     ceil_mode= False)\n",
    "\n",
    "    pooling_4: _layer = nn.MaxPool2d(kernel_size= (2, 2), stride= 2, \n",
    "                                     padding= 0, dilation= 1,\n",
    "                                     return_indices= False,\n",
    "                                     ceil_mode= False)\n",
    "\n",
    "                                    # 128\n",
    "    linear_1: _layer = nn.Linear(in_features= 128, out_features= 64, bias= True)\n",
    "    linear_2: _layer = nn.Linear(in_features= 64, out_features= 32, bias= True)\n",
    "    linear_3: _layer = nn.Linear(in_features= 32, out_features= 16, bias= True)\n",
    "    linear_4: _layer = nn.Linear(in_features= 16, out_features= 8, bias= True)\n",
    "    linear_5: _layer = nn.Linear(in_features= 8, out_features= 4, bias= True)\n",
    "    \n",
    "    '''\n",
    "    linear_1: _layer = nn.Linear(in_features= 512, out_features= 256 , bias= True)\n",
    "    linear_2: _layer = nn.Linear(in_features= 256, out_features= 128, bias= True)\n",
    "    linear_3: _layer = nn.Linear(in_features= 128, out_features= 64, bias= True)\n",
    "    linear_4: _layer = nn.Linear(in_features= 64, out_features= 32, bias= True)\n",
    "    linear_5: _layer = nn.Linear(in_features= 32, out_features= 4, bias= True)\n",
    "    '''\n",
    "\n",
    "    #@ Activation functions \n",
    "    relu: _activation = nn.ReLU()\n",
    "    tanh: _activation = nn.Tanh()\n",
    "    softmax: _activation = nn.Softmax()\n",
    "\n",
    "    #@ neuron characteristics\n",
    "    flatten = nn.Flatten()\n",
    "    dropout = nn.Dropout(p= 0.3, inplace= False)"
   ]
  },
  {
   "source": [
    "## CNN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNNModel(\n  (convolution_layers): Sequential(\n    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): Tanh()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (4): Tanh()\n    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1))\n    (10): Tanh()\n    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=128, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=64, out_features=32, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=32, out_features=16, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=16, out_features=8, bias=True)\n    (7): Tanh()\n    (8): Linear(in_features=8, out_features=4, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# CNN Model \n",
    "class CNNModel(Image_Classifier_Base):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            CNNHyperParams.convolutional_1,\n",
    "            #CNNHyperParams.relu,\n",
    "            CNNHyperParams.tanh,\n",
    "            CNNHyperParams.pooling_1,\n",
    "\n",
    "            CNNHyperParams.convolutional_2,\n",
    "            #CNNHyperParams.relu,\n",
    "            CNNHyperParams.tanh,\n",
    "            CNNHyperParams.pooling_2,\n",
    "\n",
    "            CNNHyperParams.convolutional_3,\n",
    "            CNNHyperParams.relu,\n",
    "            CNNHyperParams.pooling_3,\n",
    "\n",
    "            CNNHyperParams.convolutional_4,\n",
    "            CNNHyperParams.tanh,\n",
    "            CNNHyperParams.pooling_4\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            CNNHyperParams.linear_1,\n",
    "            CNNHyperParams.relu,\n",
    "\n",
    "            CNNHyperParams.linear_2,\n",
    "            CNNHyperParams.relu,\n",
    "\n",
    "            CNNHyperParams.linear_3,\n",
    "            CNNHyperParams.relu,\n",
    "\n",
    "            CNNHyperParams.linear_4,\n",
    "            #CNNHyperParams.relu,\n",
    "            CNNHyperParams.tanh,\n",
    "\n",
    "            CNNHyperParams.linear_5\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape)\n",
    "        x: torch.Tensor = self.convolution_layers(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x: torch.Tensor = CNNHyperParams.dropout(x)\n",
    "        x: torch.Tensor = CNNHyperParams.flatten(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #print(x.view(x.size(0), -1).shape)\n",
    "        #print(x.shape)\n",
    "        x: torch.Tensor = self.linear_layers(x)\n",
    "        #print(x.shape)\n",
    "        #return CNNHyperParams.softmax(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model: _model = CNNModel(3)\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "## Train_Test_fit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'val_loss': 1.9024345874786377, 'val_acc': 0.0}]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 2, 3, 3], but got 5-dimensional input of size [8, 1, 2, 64, 64] instead",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-c8cf2ff68057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m#############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     history += Train_Test_fit().fit_one_cycle(epochs= epochs, \n\u001b[0m\u001b[0;32m    109\u001b[0m                                              \u001b[0mmax_learning_rate\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                                              \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-c8cf2ff68057>\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, epochs, max_learning_rate, model, train_loader, val_loader, weight_decay, grad_clip, opt_function)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mlrs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-125-38872c536a96>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#print(images.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-0fc21831d795>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m#print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 2, 3, 3], but got 5-dimensional input of size [8, 1, 2, 64, 64] instead"
     ]
    }
   ],
   "source": [
    "class Train_Test_fit:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "    \n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        return {\n",
    "            item: value for item, value in zip(['module', 'name', 'objectID'], self.__repr__())\n",
    "        }\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model: _model, val_loader: _loader) -> dict[str, float]: \n",
    "        model.eval()\n",
    "        outputs: list[float] = [\n",
    "            model.validation_step(batch) for batch in val_loader\n",
    "        ]\n",
    "        return model.validation_epoch_end(outputs)\n",
    "        \n",
    "    \n",
    "    def get_learning_rate(self, optimizer: _optimizer) -> Any:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    \n",
    "    def fit_one_cycle(self, epochs: int, \n",
    "                            max_learning_rate: float, \n",
    "                            model: _model, \n",
    "                            train_loader: _loader, \n",
    "                            val_loader: _loader,  # test_loader\n",
    "                            weight_decay: Optional[float|int] = 0,\n",
    "                            grad_clip: Optional[float] = None, \n",
    "                            opt_function: Optional[_optimizer] = torch.optim.SGD) -> _text: \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        history: list = []\n",
    "        optimizer: _optimizer = opt_function(model.parameters(), max_learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "        #@: one-cycle LR scherudlar\n",
    "        sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                 max_learning_rate, \n",
    "                                                 epochs= epochs, \n",
    "                                                 steps_per_epoch= len(train_loader))\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_losses: list = []\n",
    "            lrs: list = []\n",
    "            for batch in train_loader:\n",
    "                loss: _loss = model.training_step(batch)\n",
    "\n",
    "                train_losses.append(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                if grad_clip:\n",
    "                    nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                lrs.append(self.get_learning_rate(optimizer))\n",
    "                sched.step()\n",
    "\n",
    "            #@: validation\n",
    "            result: float = self.evaluate(model, val_loader)\n",
    "            result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "            result['lrs'] = lrs\n",
    "            model.epoch_end(epoch, result)\n",
    "            history.append(result)\n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "if __name__.__contains__('__main__'):\n",
    "    path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "    sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "    training_data: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[1],\n",
    "                                            batch_size= 8)\n",
    "\n",
    "    testing_data: object  = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[0],\n",
    "                                            batch_size= 1)\n",
    "\n",
    "    training_data_loader: _loader = DataLoader(training_data, \n",
    "                                               batch_size= 1,\n",
    "                                               shuffle= True)\n",
    "\n",
    "    testing_data_loader: _loader = DataLoader(testing_data, \n",
    "                                              batch_size= 1,\n",
    "                                              shuffle= False)\n",
    "\n",
    "    model: _model = CNNModel(3)\n",
    "    #device = GPU_Acceleration(training_data_loader, 'cuda')\n",
    "    history: list[dict[str, float]] = [Train_Test_fit().evaluate(model= model, val_loader= testing_data_loader)]\n",
    "    print(history)\n",
    "\n",
    "    ##########################\n",
    "    epochs = 3\n",
    "    max_lr = 0.01\n",
    "    grad_clip = 0.1\n",
    "    weight_decay = 1e-4\n",
    "    opt_func = torch.optim.Adam\n",
    "\n",
    "\n",
    "    #############################\n",
    "\n",
    "    history += Train_Test_fit().fit_one_cycle(epochs= epochs, \n",
    "                                             max_learning_rate= max_lr, \n",
    "                                             model= model, \n",
    "                                             train_loader= training_data_loader, \n",
    "                                             val_loader= testing_data_loader, \n",
    "                                             weight_decay= weight_decay, \n",
    "                                             opt_function= opt_func)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Evaluate CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Evaluate CNN\n",
    "class EvaluateCNN:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "    \n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        return {\n",
    "            item: value for item, value in zip(['module', 'name', 'ObjectID'], self.__repr__())\n",
    "        }\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def accuracy_vs_no_of_epochs(cls, history: dict[str, float]) -> _plot:\n",
    "        accuracies: list[float] = [x['val_acc'] for x in history]\n",
    "        plt.plot(accuracies, '-x')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.title('Accuracy vs No. of epochs')\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def loss_vs_no_of_epochs(cls, history: dict[str, float]) -> _plot:\n",
    "        train_losses: list[float] = [\n",
    "            x.get('train_loss') for x in history\n",
    "        ]\n",
    "        val_losses: list[float] = [\n",
    "            x['val_loss'] for x in history\n",
    "        ]\n",
    "        plt.plot(train_losses, '-bx')\n",
    "        plt.plot(val_losses, '-rx')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.title('Loss vs No. of Epochs')\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def learning_rate_vs_batch_number(cls, history: dict[str, float]) -> _plot:\n",
    "        lrs: np.ndarray = np.concatenate([\n",
    "            x.get('lrs', []) for x in history\n",
    "        ])\n",
    "        plt.plot(lrs)\n",
    "        plt.xlabel('Batch No.')\n",
    "        plt.ylabel('Learnjing rate')\n",
    "        plt.title('Learning Rate vs Batch No.')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def model_report(cls) -> pd.DataFrame | _plot:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def confusion_matrix(cls) -> _plot:\n",
    "        ..."
   ]
  },
  {
   "source": [
    "## Save CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Save Model\n",
    "class SaveCNN:\n",
    "    def __repr__(self) -> tuple[str, ...]:\n",
    "        return self.__module__, type(self).__name__, hex(id(self))\n",
    "    \n",
    "\n",
    "    def __str__(self) -> dict[str, str]:\n",
    "        return {\n",
    "            item: value for item, value in zip(['module', 'name', 'ObjectID'], self.__repr__())\n",
    "        }\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, model: _model, model_name: Optional[str] = 'BrainMRI_Model.pt') -> None:\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def is_save_model(cls, model: _model) -> bool:\n",
    "        ...\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, new_model: _model, old_model: str) -> _model:\n",
    "        new_model.load_state_dict(torch.load(old_model))\n",
    "        return new_model\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def is_load_model(cls, new_model: _model, old_model: _model) -> bool:\n",
    "        ...\n"
   ]
  },
  {
   "source": [
    "# Driver code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__.__contains__(\"__main_module__\"):\n",
    "\n",
    "    path: _path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\BRIAN_unit_testing\\\\\"\n",
    "    sub_path: list[str] = [\"Testing\", \"Training\"]\n",
    "\n",
    "    training_data: object = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[1],\n",
    "                                            batch_size= 8)\n",
    "\n",
    "    testing_data: object  = BrainMRIDataset(path= path, \n",
    "                                            sub_path= sub_path[0],\n",
    "                                            batch_size= 1)\n",
    "\n",
    "    training_data_loader: _loader = DataLoader(training_data, \n",
    "                                               batch_size= 8,\n",
    "                                               shuffle= True)\n",
    "\n",
    "    testing_data_loader: _loader = DataLoader(testing_data, \n",
    "                                              batch_size= 1,\n",
    "                                              shuffle= False)\n",
    "\n",
    "    model: _model = CNNModel(3)\n",
    "    device = GPU_Acceleration(training_data_loader, 'cuda')\n",
    "    for item in testing_data_loader:\n",
    "        print(item.squeeze_())\n",
    "\n",
    "    #history: list[dict[str, float]] = [Train_Test_fit().evaluate(model= model, val_loader= testing_data_loader)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}